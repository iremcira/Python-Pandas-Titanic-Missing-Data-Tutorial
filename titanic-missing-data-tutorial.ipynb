{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4d7aa0",
   "metadata": {},
   "source": [
    "# Python-Pandas-Titanic-Missing-Data-Tutorial\n",
    "\n",
    "**A comprehensive, step-by-step guide to handling missing data in the Titanic dataset using Python's Pandas library.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420bb23f",
   "metadata": {},
   "source": [
    "### 1. Setting Up Your Environment and Importing Libraries\n",
    "\n",
    "Before we start working with data, we need to make sure our Python environment is ready. This involves importing the necessary libraries that provide the tools we'll use for data manipulation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a2668",
   "metadata": {},
   "source": [
    "#### Step 1.1: Import Essential Libraries\n",
    "\n",
    "**Explanation:**\n",
    "We import pandas for data manipulation, numpy for numerical operations (especially useful for handling NaN values), matplotlib.pyplot for basic plotting, and seaborn for creating more aesthetically pleasing and informative statistical graphics. These are the cornerstones of almost any data science project in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61ac6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to install libraries first:\n",
    "# %pip install pandas numpy matplotlib seaborn\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a style for seaborn plots for better aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "# This ensures that your plots look good right out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080768b",
   "metadata": {},
   "source": [
    "**The Logic Behind**\n",
    "    \n",
    "    pandas: It's the primary tool for working with tabular data (like CSV files) in Python. It provides DataFrames, which are highly efficient and flexible for data handling.\n",
    "\n",
    "    numpy: Often works hand-in-hand with Pandas for numerical computations, especially when dealing with missing values (e.g., np.nan).\n",
    "\n",
    "    matplotlib.pyplot & seaborn: Data visualization is crucial for understanding the distribution of missing values and for exploring relationships in the data before and after cleaning. Seaborn builds on Matplotlib to create more visually appealing plots with less code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e3cb9",
   "metadata": {},
   "source": [
    "### 2. Loading the Dataset\n",
    "We are taking our train.csv file from disk storage and \"loading\" or \"reading\" its contents into our Python environment. Specifically, we're converting it into a Pandas DataFrame, which is an in-memory, structured representation of our data, making it ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b15680",
   "metadata": {},
   "source": [
    "#### Step 2.1: Define File Path\n",
    "\n",
    "**Explanation:**\n",
    "We first define the file_path variable to point to our train.csv file. If the CSV file is in the same directory as your Jupyter Notebook, just the filename is enough. If it's in a subdirectory (e.g., data/), you'd specify data/train.csv. For files located elsewhere, you'd use the absolute path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a9f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train.csv' is in the same directory as this Jupyter Notebook\n",
    "file_path = 'train.csv'\n",
    "\n",
    "# If your CSV is in a 'data' folder:\n",
    "# file_path = 'data/train.csv'\n",
    "\n",
    "# Or an absolute path (Windows example, note 'r' for raw string or use forward slashes):\n",
    "# file_path = r'C:\\Users\\YourUser\\Documents\\PythonProjects\\TitanicData\\train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51417f9",
   "metadata": {},
   "source": [
    "**The Logic Behind**\n",
    "\n",
    "    Defining the file path makes our code more readable and easier to modify if the file location changes. It's a good practice to keep paths as variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724ab3f",
   "metadata": {},
   "source": [
    "#### Step 2.2: Read the CSV File into a Pandas DataFrame\n",
    "\n",
    "**Explanation:**\n",
    "The pd.read_csv() function is the workhorse here. It reads the comma-separated values from our train.csv file and transforms them into a tabular DataFrame object in our computer's memory. We store this DataFrame in a variable typically named df. We also include a try-except block for robust error handling, informing the user if the file isn't found or if there's another issue during reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca22571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully read and loaded into a DataFrame!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"CSV file successfully read and loaded into a DataFrame!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please check the path or ensure the file is in the correct directory.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"Error: The file '{file_path}' is empty. Make sure it contains data.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2a0ca",
   "metadata": {},
   "source": [
    "**The Logic Behind**\n",
    "\n",
    "    This is the core step to get our raw data into a usable format for Python. The Pandas DataFrame is optimized for data analysis tasks, allowing for efficient querying, manipulation, and computation. The try-except block makes our code more robust and user-friendly, preventing crashes due to common issues like a missing file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f662f4",
   "metadata": {},
   "source": [
    "### 3. Initial Data Inspection and Understanding Missing Values\n",
    "\n",
    "After loading the data, it's crucial to get a first impression. We'll check the basic structure, data types, and, most importantly for this tutorial, identify where and how many missing values exist in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562962a",
   "metadata": {},
   "source": [
    "#### Step 3.1: Display the First Few Rows (df.head())\n",
    "\n",
    "**Explanation:**\n",
    "df.head() prints the first 5 rows of our DataFrame by default. This gives us a quick glimpse of the data, column names, and the type of values they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b721c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of the DataFrame:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 5 rows of the DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928cb78f",
   "metadata": {},
   "source": [
    "**The Logic Behind**\n",
    "\n",
    "    df.head() is essential for sanity checks. It helps confirm that the data loaded correctly, the columns are as expected, and we can quickly spot obvious data issues or unexpected formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b91e60",
   "metadata": {},
   "source": [
    "#### Step 3.2: Get Summary Information About the DataFrame (df.info())\n",
    "\n",
    "**Explanation:**\n",
    "df.info() provides a concise summary of the DataFrame. It shows:\n",
    "\n",
    "--The number of entries (rows).\n",
    "\n",
    "--The total number of columns.\n",
    "\n",
    "--Each column's name, the count of non-null values, and its data type (dtype).\n",
    "\n",
    "--Memory usage.\n",
    "\n",
    "The \"Non-Null Count\" is especially important for identifying missing values: if it's less than the total number of entries, that column contains missing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978903a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "General information about the DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGeneral information about the DataFrame:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867444c",
   "metadata": {},
   "source": [
    "**The Logic Behind**\n",
    "    \n",
    "    df.info() is a powerful first diagnostic tool. It quickly reveals data types (e.g., object for strings, int64 for integers, float64 for decimals), which is crucial for subsequent operations. More importantly, it directly tells us which columns have missing values and how many, without needing further calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50601e32",
   "metadata": {},
   "source": [
    "#### Step 3.3: Calculate the Number and Percentage of Missing Values (df.isnull().sum())\n",
    "\n",
    "Explanation:\n",
    "This is the most direct way to quantify missing values.\n",
    "\n",
    "-- df.isnull(): Returns a DataFrame of boolean values (True for missing, False for not missing) for every cell.\n",
    "\n",
    "-- .sum(): When applied to the boolean DataFrame, True is treated as 1 and False as 0. So, sum() for each column counts the number of True values, effectively counting the missing values in each column.\n",
    "\n",
    "-- Dividing by len(df) (total rows) and multiplying by 100 gives the percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b96931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing values per column:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "Percentage of missing values per column:\n",
      "PassengerId     0.00\n",
      "Survived        0.00\n",
      "Pclass          0.00\n",
      "Name            0.00\n",
      "Sex             0.00\n",
      "Age            19.87\n",
      "SibSp           0.00\n",
      "Parch           0.00\n",
      "Ticket          0.00\n",
      "Fare            0.00\n",
      "Cabin          77.10\n",
      "Embarked        0.22\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNumber of missing values per column:\")\n",
    "missing_values_count = df.isnull().sum()\n",
    "print(missing_values_count)\n",
    "\n",
    "print(\"\\nPercentage of missing values per column:\")\n",
    "missing_values_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(missing_values_percentage.round(2)) # Round to 2 decimal places for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf69755",
   "metadata": {},
   "source": [
    "**The Logic Behind**\n",
    "\n",
    "Knowing the exact count and percentage of missing values per column is critical for deciding on the appropriate strategy for handling them. For example, a column with 90% missing values might be dropped, while a column with 5% missing values might be imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce14c8",
   "metadata": {},
   "source": [
    "4. Visualizing Missing Data Patterns\n",
    "\n",
    "What are we doing here?\n",
    "\n",
    "Beyond just numbers, visualizing missing data helps us understand the patterns of missingness. Are values missing randomly? Are they missing in specific columns together? Visuals provide insights that raw numbers might miss.\n",
    "\n",
    "Step 4.1: Visualize Missing Data with a Heatmap (Seaborn)\n",
    "\n",
    "Explanation:\n",
    "A heatmap can visually represent where missing values are located across your DataFrame.\n",
    "\n",
    "    df.isnull(): Again, creates a boolean DataFrame (True for missing).\n",
    "\n",
    "    sns.heatmap(): Creates a heatmap from this boolean DataFrame.\n",
    "\n",
    "    cbar=False: Hides the color bar as we're only interested in presence/absence.\n",
    "\n",
    "    cmap='viridis' (or 'rocket', 'flare'): Sets the color scheme. Different colors can represent missing vs. non-missing values clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fff92e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
